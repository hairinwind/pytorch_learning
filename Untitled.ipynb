{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 40138530.3439616\n",
      "1 53657452.213567\n",
      "2 77339950.16397685\n",
      "3 81123974.6650319\n",
      "4 45725237.57341118\n",
      "5 11758108.560189066\n",
      "6 3008434.9710800187\n",
      "7 1737441.1592201483\n",
      "8 1376725.2357491697\n",
      "9 1148221.9445042722\n",
      "10 971253.7414247927\n",
      "11 829316.9918905043\n",
      "12 713885.4979469918\n",
      "13 619195.7758853682\n",
      "14 540439.4956144913\n",
      "15 474342.44863551005\n",
      "16 418433.9485186151\n",
      "17 370891.861795267\n",
      "18 330129.9465293534\n",
      "19 294978.29087434127\n",
      "20 264522.9643182534\n",
      "21 237951.1877029059\n",
      "22 214714.07614229625\n",
      "23 194311.41152385168\n",
      "24 176295.47944063527\n",
      "25 160325.59315805172\n",
      "26 146118.95287209016\n",
      "27 133431.4105321026\n",
      "28 122082.17964651031\n",
      "29 111907.77383518656\n",
      "30 102750.92874944859\n",
      "31 94485.13013136383\n",
      "32 87008.19886650605\n",
      "33 80230.67640870268\n",
      "34 74082.60586070965\n",
      "35 68491.85915500319\n",
      "36 63393.990646901395\n",
      "37 58737.08512998747\n",
      "38 54477.98233254285\n",
      "39 50574.71338769396\n",
      "40 46992.76456903558\n",
      "41 43701.68103172023\n",
      "42 40674.575883119396\n",
      "43 37886.52012811681\n",
      "44 35317.23791955193\n",
      "45 32946.09145317591\n",
      "46 30755.360633624285\n",
      "47 28728.4734464816\n",
      "48 26852.98284481698\n",
      "49 25116.957932856618\n",
      "50 23507.38074371515\n",
      "51 22012.49028680262\n",
      "52 20623.834406674527\n",
      "53 19333.08704440089\n",
      "54 18131.889258093535\n",
      "55 17014.447470506708\n",
      "56 15973.292474632453\n",
      "57 15002.63862728736\n",
      "58 14096.724472487778\n",
      "59 13251.460108711359\n",
      "60 12461.717201288448\n",
      "61 11723.56687231822\n",
      "62 11033.269104666433\n",
      "63 10387.406060048317\n",
      "64 9782.928739549927\n",
      "65 9216.871781556083\n",
      "66 8686.834612563387\n",
      "67 8189.823365143034\n",
      "68 7723.5946924367945\n",
      "69 7286.341564550521\n",
      "70 6875.9084478435725\n",
      "71 6490.705916412908\n",
      "72 6129.80009032455\n",
      "73 5790.697209100513\n",
      "74 5471.7031304779075\n",
      "75 5171.76066994222\n",
      "76 4889.520512932122\n",
      "77 4623.832727237617\n",
      "78 4373.572048902854\n",
      "79 4137.965764849936\n",
      "80 3915.9544848779756\n",
      "81 3706.7636311936226\n",
      "82 3509.597126729748\n",
      "83 3323.970986096894\n",
      "84 3148.7304862182737\n",
      "85 2983.358850982585\n",
      "86 2827.367341074475\n",
      "87 2680.0755732286652\n",
      "88 2540.90801765581\n",
      "89 2409.48047744087\n",
      "90 2285.1997116331677\n",
      "91 2167.782267858067\n",
      "92 2056.733555232954\n",
      "93 1951.7003309804431\n",
      "94 1852.3265747578482\n",
      "95 1758.3558201946043\n",
      "96 1669.3509114342755\n",
      "97 1585.1181134114354\n",
      "98 1505.3668172452299\n",
      "99 1429.8301101057596\n",
      "100 1358.2860916867007\n",
      "101 1290.5216530872308\n",
      "102 1226.290005072446\n",
      "103 1165.4165292883586\n",
      "104 1107.7099642179653\n",
      "105 1053.049469843961\n",
      "106 1001.3052945964414\n",
      "107 952.226502418366\n",
      "108 905.6538833489535\n",
      "109 861.4597440857984\n",
      "110 819.5299625300245\n",
      "111 779.7291360667698\n",
      "112 741.9431908802917\n",
      "113 706.0633002677134\n",
      "114 671.9833645106914\n",
      "115 639.6197533541783\n",
      "116 608.878034767323\n",
      "117 579.6729845568409\n",
      "118 551.9311969781744\n",
      "119 525.5610023486879\n",
      "120 500.49133297830383\n",
      "121 476.66687566304904\n",
      "122 454.0220382511712\n",
      "123 432.48946605087326\n",
      "124 412.01595454787537\n",
      "125 392.5462264144484\n",
      "126 374.0240659338246\n",
      "127 356.4079097546504\n",
      "128 339.64715279366004\n",
      "129 323.697020790165\n",
      "130 308.5231955199621\n",
      "131 294.0834597279131\n",
      "132 280.33879648709126\n",
      "133 267.25651107356913\n",
      "134 254.80644330364896\n",
      "135 242.95402614564333\n",
      "136 231.67022658802478\n",
      "137 220.9226846236726\n",
      "138 210.69096676851188\n",
      "139 200.9438308518681\n",
      "140 191.6629147949822\n",
      "141 182.8235855129721\n",
      "142 174.40275852968983\n",
      "143 166.38233330893559\n",
      "144 158.73990604072029\n",
      "145 151.45821268718802\n",
      "146 144.52232856212802\n",
      "147 137.91010873156762\n",
      "148 131.60646820913223\n",
      "149 125.60203936160326\n",
      "150 119.87500998258022\n",
      "151 114.41804458095365\n",
      "152 109.21412362200869\n",
      "153 104.25391627467658\n",
      "154 99.52339369647024\n",
      "155 95.01273729926336\n",
      "156 90.7105315060048\n",
      "157 86.60886476617839\n",
      "158 82.69616439596089\n",
      "159 78.96413183846413\n",
      "160 75.40435373712258\n",
      "161 72.00854294399164\n",
      "162 68.76901674168984\n",
      "163 65.67873424851444\n",
      "164 62.73038755879841\n",
      "165 59.916820378538496\n",
      "166 57.23221996843807\n",
      "167 54.6702748001365\n",
      "168 52.22515337787516\n",
      "169 49.89186286202056\n",
      "170 47.66452707976366\n",
      "171 45.539010905705595\n",
      "172 43.50959588292084\n",
      "173 41.57285235567511\n",
      "174 39.72347267633138\n",
      "175 37.958226695236604\n",
      "176 36.27281321854639\n",
      "177 34.66354860891338\n",
      "178 33.1267577369597\n",
      "179 31.65961041776307\n",
      "180 30.25907764111468\n",
      "181 28.920839786809495\n",
      "182 27.64306617041711\n",
      "183 26.422901724493798\n",
      "184 25.25740151433775\n",
      "185 24.144326160813407\n",
      "186 23.081079797049295\n",
      "187 22.065282502970533\n",
      "188 21.095210832644888\n",
      "189 20.16839618326629\n",
      "190 19.282924237868944\n",
      "191 18.437068268560623\n",
      "192 17.628811753561827\n",
      "193 16.856427406492116\n",
      "194 16.118535340096777\n",
      "195 15.41329738706513\n",
      "196 14.739751618871505\n",
      "197 14.095748707667283\n",
      "198 13.480606605290166\n",
      "199 12.892551150548464\n",
      "200 12.330462335486093\n",
      "201 11.793434550144113\n",
      "202 11.280142268468529\n",
      "203 10.789394151994614\n",
      "204 10.320293971490315\n",
      "205 9.872998243666466\n",
      "206 9.445675293208728\n",
      "207 9.037173512337407\n",
      "208 8.646637923770164\n",
      "209 8.273167733272658\n",
      "210 7.916193171724639\n",
      "211 7.574800678926929\n",
      "212 7.248381951152476\n",
      "213 6.936221940574166\n",
      "214 6.637697811303063\n",
      "215 6.352239332278929\n",
      "216 6.0792650828293295\n",
      "217 5.818138492230704\n",
      "218 5.568387042617447\n",
      "219 5.329612103294721\n",
      "220 5.101121066051865\n",
      "221 4.882559949317155\n",
      "222 4.673598909601987\n",
      "223 4.473588838301852\n",
      "224 4.282327334995129\n",
      "225 4.099385564627415\n",
      "226 3.924354302884612\n",
      "227 3.756888093259383\n",
      "228 3.5966369363330157\n",
      "229 3.443362861897066\n",
      "230 3.296655332246443\n",
      "231 3.1562815061623484\n",
      "232 3.0220128989995683\n",
      "233 2.8934786130705814\n",
      "234 2.7705129774854624\n",
      "235 2.652870471481336\n",
      "236 2.540274025439952\n",
      "237 2.4324959425423245\n",
      "238 2.329361306208153\n",
      "239 2.2306916611046\n",
      "240 2.136241071455747\n",
      "241 2.0458198764530153\n",
      "242 1.9592950507109643\n",
      "243 1.8764619891662024\n",
      "244 1.7971841681036227\n",
      "245 1.7212889115125478\n",
      "246 1.6486697553674978\n",
      "247 1.5791165130778446\n",
      "248 1.5125428802731067\n",
      "249 1.4488296087858514\n",
      "250 1.387815804101788\n",
      "251 1.3294088242708038\n",
      "252 1.273508142614608\n",
      "253 1.2199840769904022\n",
      "254 1.1687237783288138\n",
      "255 1.1196400893997094\n",
      "256 1.0726527580482699\n",
      "257 1.0276609294168055\n",
      "258 0.9845886243390201\n",
      "259 0.9433477820803654\n",
      "260 0.9038391856038586\n",
      "261 0.8660068869794769\n",
      "262 0.829775339364781\n",
      "263 0.7950902506360569\n",
      "264 0.7618619716285059\n",
      "265 0.7300464127420754\n",
      "266 0.6995703431761566\n",
      "267 0.6703847944356623\n",
      "268 0.6424271460336111\n",
      "269 0.6156514999064715\n",
      "270 0.5899996063345199\n",
      "271 0.5654367775581206\n",
      "272 0.5419063157166545\n",
      "273 0.519361955160698\n",
      "274 0.49777357763887864\n",
      "275 0.4770973388358981\n",
      "276 0.45727984237462904\n",
      "277 0.4382934774738684\n",
      "278 0.4201160020415493\n",
      "279 0.40269628933917806\n",
      "280 0.3860052225722523\n",
      "281 0.3700147992879206\n",
      "282 0.3546984998820307\n",
      "283 0.3400154711101673\n",
      "284 0.3259501879197102\n",
      "285 0.3124761196876896\n",
      "286 0.2995629301018201\n",
      "287 0.2871879751903495\n",
      "288 0.27533074478771713\n",
      "289 0.2639724816525114\n",
      "290 0.25308239124307713\n",
      "291 0.2426494087815822\n",
      "292 0.23265684691674143\n",
      "293 0.22307740488536473\n",
      "294 0.21389315207374718\n",
      "295 0.20509264087511514\n",
      "296 0.19665782605127943\n",
      "297 0.1885752746425624\n",
      "298 0.18082830175889883\n",
      "299 0.17340128721813447\n",
      "300 0.16628495852839947\n",
      "301 0.1594625080810272\n",
      "302 0.15292399187498285\n",
      "303 0.14665539855789025\n",
      "304 0.1406495682564425\n",
      "305 0.13489054086694968\n",
      "306 0.12936954232275866\n",
      "307 0.12407627579527128\n",
      "308 0.1190036629724335\n",
      "309 0.11413870345992028\n",
      "310 0.10947478352008683\n",
      "311 0.1050047849812245\n",
      "312 0.10071863943181214\n",
      "313 0.09660945305760144\n",
      "314 0.09266919988957302\n",
      "315 0.08889287124274262\n",
      "316 0.0852704016254962\n",
      "317 0.08179734176596018\n",
      "318 0.07846806672963948\n",
      "319 0.07527660334992192\n",
      "320 0.07221486298857663\n",
      "321 0.06927932818178745\n",
      "322 0.06646436543711674\n",
      "323 0.0637646027977721\n",
      "324 0.06117598989413184\n",
      "325 0.0586933453476484\n",
      "326 0.056313695445709716\n",
      "327 0.05403018195506336\n",
      "328 0.05184052830817953\n",
      "329 0.04974022881466314\n",
      "330 0.047726451871484325\n",
      "331 0.045794874905888974\n",
      "332 0.04394229348465663\n",
      "333 0.04216552690563821\n",
      "334 0.04046146635739077\n",
      "335 0.038826687214343014\n",
      "336 0.037258281836891796\n",
      "337 0.03575435635994934\n",
      "338 0.03431187227791006\n",
      "339 0.0329276778489841\n",
      "340 0.03159988565682392\n",
      "341 0.030326760882393534\n",
      "342 0.029105400981550053\n",
      "343 0.027933236673080154\n",
      "344 0.0268087918903459\n",
      "345 0.025730297423739668\n",
      "346 0.02469560219477196\n",
      "347 0.02370294419886894\n",
      "348 0.022750670355070855\n",
      "349 0.021836846158138717\n",
      "350 0.020960032345541023\n",
      "351 0.02011880164504471\n",
      "352 0.019311773836280383\n",
      "353 0.018537571508549504\n",
      "354 0.017794606818418348\n",
      "355 0.017081575486731733\n",
      "356 0.016397339825094066\n",
      "357 0.015740954130015014\n",
      "358 0.015111206205761596\n",
      "359 0.014506806920538757\n",
      "360 0.01392661888532371\n",
      "361 0.013370256741169144\n",
      "362 0.012836054991859623\n",
      "363 0.01232336347641862\n",
      "364 0.011831309831633857\n",
      "365 0.011359348283697956\n",
      "366 0.01090617486409282\n",
      "367 0.010471201234902056\n",
      "368 0.010053872597166\n",
      "369 0.00965337028388863\n",
      "370 0.009268816219636126\n",
      "371 0.008899726006613574\n",
      "372 0.00854559363767675\n",
      "373 0.008205639723517335\n",
      "374 0.007879340672485007\n",
      "375 0.007566186015731514\n",
      "376 0.007265560568613427\n",
      "377 0.0069769552894686174\n",
      "378 0.006699894507228107\n",
      "379 0.006433973507643267\n",
      "380 0.0061786911166830365\n",
      "381 0.005933649811009623\n",
      "382 0.005698421336040127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383 0.005472601382467706\n",
      "384 0.0052557622309191325\n",
      "385 0.00504768140243694\n",
      "386 0.004847828977205612\n",
      "387 0.004655958731592181\n",
      "388 0.004471762301490717\n",
      "389 0.004295003825586226\n",
      "390 0.004125293352061173\n",
      "391 0.003962268643506404\n",
      "392 0.003805767615384509\n",
      "393 0.0036555199713153696\n",
      "394 0.0035112328446222432\n",
      "395 0.003372657516676073\n",
      "396 0.003239673044283369\n",
      "397 0.0031119202154453103\n",
      "398 0.002989256320882244\n",
      "399 0.002871461847353352\n",
      "400 0.0027583846186869185\n",
      "401 0.002649768895742302\n",
      "402 0.002545465635768211\n",
      "403 0.0024453144077092393\n",
      "404 0.0023491796861800874\n",
      "405 0.002256810699168381\n",
      "406 0.002168098436258291\n",
      "407 0.0020829083086695054\n",
      "408 0.0020011182966709214\n",
      "409 0.00192254531725721\n",
      "410 0.0018470815440780066\n",
      "411 0.001774602308166609\n",
      "412 0.0017049927592726612\n",
      "413 0.001638147816865468\n",
      "414 0.0015739349235955782\n",
      "415 0.001512257006787831\n",
      "416 0.001453017821535692\n",
      "417 0.0013961361604261622\n",
      "418 0.0013414862807143383\n",
      "419 0.0012889932133490858\n",
      "420 0.0012385865215956194\n",
      "421 0.0011901692882571335\n",
      "422 0.0011436376961464388\n",
      "423 0.0010989361609494912\n",
      "424 0.0010560082689578256\n",
      "425 0.0010147765992790227\n",
      "426 0.000975154973830499\n",
      "427 0.0009370882195551353\n",
      "428 0.0009005408044840268\n",
      "429 0.0008654129151738558\n",
      "430 0.0008316629065052807\n",
      "431 0.0007992412885773031\n",
      "432 0.000768103527549736\n",
      "433 0.0007381891815481073\n",
      "434 0.0007094423821991966\n",
      "435 0.0006818217267993701\n",
      "436 0.0006552899164326771\n",
      "437 0.0006297965981556335\n",
      "438 0.0006052999489048548\n",
      "439 0.0005817649360103926\n",
      "440 0.0005591525402012202\n",
      "441 0.0005374248096775168\n",
      "442 0.0005165471887011125\n",
      "443 0.0004964880136229476\n",
      "444 0.0004772116571501762\n",
      "445 0.00045869210416513595\n",
      "446 0.0004408977765560729\n",
      "447 0.0004237941181291712\n",
      "448 0.00040736567192861054\n",
      "449 0.0003915780519755021\n",
      "450 0.00037641173929083547\n",
      "451 0.0003618276695618175\n",
      "452 0.00034781208842882933\n",
      "453 0.00033434955047996186\n",
      "454 0.00032140768754247484\n",
      "455 0.00030896935879449637\n",
      "456 0.00029701478699577905\n",
      "457 0.00028553448344305744\n",
      "458 0.0002744930188539406\n",
      "459 0.0002638831154158903\n",
      "460 0.0002536853264318057\n",
      "461 0.00024388987051010308\n",
      "462 0.00023447017571040244\n",
      "463 0.00022542116362903866\n",
      "464 0.0002167204113338716\n",
      "465 0.00020836125434276032\n",
      "466 0.00020032350213931594\n",
      "467 0.00019259773979885247\n",
      "468 0.00018517327951520353\n",
      "469 0.00017803911957807233\n",
      "470 0.00017117817464521926\n",
      "471 0.00016458359160250965\n",
      "472 0.00015824612926774182\n",
      "473 0.0001521544326046782\n",
      "474 0.00014629715496376224\n",
      "475 0.00014066847978059399\n",
      "476 0.00013525702807322493\n",
      "477 0.00013005578150476538\n",
      "478 0.00012505695050628348\n",
      "479 0.00012025358007398192\n",
      "480 0.00011563242474468839\n",
      "481 0.00011119063365750055\n",
      "482 0.0001069195194510517\n",
      "483 0.00010281455550746844\n",
      "484 9.886737585509492e-05\n",
      "485 9.507346051031605e-05\n",
      "486 9.142553940504992e-05\n",
      "487 8.791952000488993e-05\n",
      "488 8.45473167426172e-05\n",
      "489 8.130595540320393e-05\n",
      "490 7.819012021665619e-05\n",
      "491 7.519469872442143e-05\n",
      "492 7.231345592850305e-05\n",
      "493 6.954483962979205e-05\n",
      "494 6.688238957212414e-05\n",
      "495 6.432266747719507e-05\n",
      "496 6.18606069100611e-05\n",
      "497 5.9493555599270266e-05\n",
      "498 5.721822322832309e-05\n",
      "499 5.503051635972303e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    # dot is doing Matrix multiplication\n",
    "    # [64, 1000].[1000, 100] = [64, 100]\n",
    "    \n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    # h_relu.T is Matix transpose\n",
    "#     print(\"h_relu.T.shape\", h_relu.T.shape)\n",
    "#     print(\"grad_y_pred.shape\", grad_y_pred.shape)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "#     print('grad_w2.shape', grad_w2.shape);\n",
    "#     print('w2.shape', w2.shape)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "#     print('grad_h_relu.shape', grad_h_relu.shape)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n",
      "0 29408370.0\n",
      "torch.Size([64, 100])\n",
      "1 23900436.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(2):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    print(x.mm(w1).shape)\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
